#!/usr/bin/env python3
"""
å®Ÿéš›ã®LLMæ¥ç¶šã‚’è¡Œã†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
llm_generator.pyã®_call_llmãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç½®ãæ›ãˆã¾ã™
"""

import requests
import json
import os

def create_ollama_version():
    """ollamaç‰ˆã®_call_llmãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”Ÿæˆ"""
    ollama_code = '''    def _call_llm(self, prompt: str) -> str:
        """ollamaï¼ˆãƒ­ãƒ¼ã‚«ãƒ«AIï¼‰ã‚’å‘¼ã³å‡ºã—"""
        try:
            response = requests.post("http://localhost:11434/api/generate", json={
                "model": self.config.model_name,
                "prompt": prompt,
                "temperature": self.config.temperature,
                "stream": False
            }, timeout=120)
            
            if response.status_code == 200:
                return response.json()["response"]
            else:
                raise Exception(f"ollama APIã‚¨ãƒ©ãƒ¼: {response.status_code}")
                
        except Exception as e:
            print(f"LLMå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ¢ç”¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._demo_response()'''
    
    return ollama_code

def create_openai_version():
    """OpenAIç‰ˆã®_call_llmãƒ¡ã‚½ãƒƒãƒ‰ã‚’ç”Ÿæˆ"""
    openai_code = '''    def _call_llm(self, prompt: str) -> str:
        """OpenAI APIã‚’å‘¼ã³å‡ºã—"""
        try:
            import openai
            client = openai.OpenAI()
            
            response = client.chat.completions.create(
                model=self.config.model_name or "gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"LLMå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ¢ç”¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._demo_response()'''
    
    return openai_code

def backup_original():
    """å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
    import shutil
    shutil.copy("llm_generator.py", "llm_generator.py.backup")
    print("âœ… å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ: llm_generator.py.backup")

def replace_llm_method(method_type="ollama"):
    """_call_llmãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿéš›ã®LLMå‘¼ã³å‡ºã—ã«ç½®ãæ›ãˆ"""
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
    backup_original()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    with open("llm_generator.py", "r", encoding="utf-8") as f:
        content = f.read()
    
    # ç½®ãæ›ãˆã‚‹ã‚³ãƒ¼ãƒ‰ã‚’é¸æŠ
    if method_type == "ollama":
        new_method = create_ollama_version()
        import_line = "import requests"
    elif method_type == "openai":
        new_method = create_openai_version()
        import_line = "import openai"
    else:
        print("âŒ æœªå¯¾å¿œã®method_type")
        return False
    
    # importã®è¿½åŠ 
    if import_line not in content:
        content = content.replace("import os", f"import os\n{import_line}")
    
    # ãƒ‡ãƒ¢ç”¨_call_llmãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ¢ã—ã¦ç½®ãæ›ãˆ
    start_marker = '    def _call_llm(self, prompt: str) -> str:'
    end_marker = '        return self._demo_response()'
    
    start_pos = content.find(start_marker)
    if start_pos == -1:
        print("âŒ _call_llmãƒ¡ã‚½ãƒƒãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False
    
    # end_markerã®æ¬¡ã®è¡Œã¾ã§ã‚’è¦‹ã¤ã‘ã‚‹
    end_pos = content.find(end_marker, start_pos)
    if end_pos == -1:
        print("âŒ ãƒ¡ã‚½ãƒƒãƒ‰ã®çµ‚äº†ä½ç½®ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False
    
    end_pos = content.find('\n', end_pos) + 1
    
    # ç½®ãæ›ãˆå®Ÿè¡Œ
    new_content = content[:start_pos] + new_method + content[end_pos:]
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    with open("llm_generator.py", "w", encoding="utf-8") as f:
        f.write(new_content)
    
    print(f"âœ… {method_type}ç‰ˆã®LLMå‘¼ã³å‡ºã—ã«ç½®ãæ›ãˆã¾ã—ãŸ")
    return True

def test_connection(method_type="ollama"):
    """æ¥ç¶šãƒ†ã‚¹ãƒˆ"""
    if method_type == "ollama":
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                print("âœ… ollamaæ¥ç¶šç¢ºèª")
                return True
            else:
                print("âŒ ollamaæ¥ç¶šå¤±æ•—")
                return False
        except:
            print("âŒ ollamaãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“")
            return False
            
    elif method_type == "openai":
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            print("âœ… OpenAI APIã‚­ãƒ¼ç¢ºèª")
            return True
        else:
            print("âŒ OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=== LLMæ¥ç¶šã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— ===")
    print("1. ollamaï¼ˆãƒ­ãƒ¼ã‚«ãƒ«AIã€ç„¡æ–™ï¼‰")
    print("2. OpenAIï¼ˆChatGPTã€æœ‰æ–™ï¼‰")
    print("3. ã‚­ãƒ£ãƒ³ã‚»ãƒ«")
    
    choice = input("é¸æŠã—ã¦ãã ã•ã„ (1-3): ").strip()
    
    if choice == "1":
        method = "ollama"
        print("\nollamaã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’é¸æŠ")
        if not test_connection("ollama"):
            print("âš ï¸  ollamaãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“")
            print("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †:")
            print("1. https://ollama.ai ã‹ã‚‰ollamaã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
            print("2. ollama pull qwen2.5:7b")
            print("3. ollama serve")
            print("4. å†åº¦ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ")
            return
            
    elif choice == "2":
        method = "openai"
        print("\nOpenAI APIã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’é¸æŠ")
        if not test_connection("openai"):
            print("âš ï¸  OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            print("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †:")
            print("1. https://platform.openai.com ã§APIã‚­ãƒ¼å–å¾—")
            print("2. export OPENAI_API_KEY='your-api-key'")
            print("3. pip install openai")
            print("4. å†åº¦ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ")
            return
    else:
        print("ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
        return
    
    # å®Ÿéš›ã®ç½®ãæ›ãˆå®Ÿè¡Œ
    if replace_llm_method(method):
        print(f"\nğŸ‰ {method}ç‰ˆã¸ã®æ¥ç¶šå®Œäº†!")
        print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("python3 main.py --demo  # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
        print("python3 main.py --theme 'å®‰å…¨æ•™è‚²'  # å®Ÿéš›ã®ç”Ÿæˆ")

if __name__ == "__main__":
    main()